{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Development code - SVM\n",
    "\n",
    "We start development by using a smaller and more manageable data set to figure out what a piece of code is actually doing or to test out a new piece of code before applying it to larger datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Training Data:\n",
      "\n",
      "[[ 0.57442478  1.0679959  -0.50499719 -0.83689563]\n",
      " [-1.0161851  -0.07748431 -1.38694829  0.09319964]\n",
      " [ 0.25843924  0.39654861  0.8764847   1.99541077]\n",
      " [-0.8556697   0.53403462 -0.67092401  0.18207436]\n",
      " [ 0.02367122 -1.11203876  0.26283394 -0.27124496]]\n",
      "\n",
      " Parameters:\n",
      "[[-0.31691947 -0.39810944  0.85919763]\n",
      " [ 0.95308924 -0.63623584  1.56948124]\n",
      " [ 0.67043001 -0.20444275  0.19902715]\n",
      " [ 0.78041851  0.89132826 -0.43489747]]\n",
      "\n",
      "True Labels:\n",
      "[1 2 0 0 1]\n",
      "\n",
      "Score (XW):\n",
      "[[-0.15584511 -1.5508869   2.43319958]\n",
      " [-0.60891765  0.82047416 -1.31128664]\n",
      " [ 2.44091895  1.24418953  0.15107114]\n",
      " [ 0.47244765  0.30033179 -0.10974783]\n",
      " [-1.10284687  0.40259238 -1.55471088]]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Evaluate the naive implementation of the loss we provided for you:\n",
    "from cs231n.classifiers.linear_svm_V1 import svm_loss_naive\n",
    "\n",
    "# Use a smaller dataset to code svm_loss_naive \n",
    "# We will first try a small training dataset with only 5 data points and 3 classes\n",
    "X1 = np.random.randn(5, 4)\n",
    "W1 = np.random.randn(4, 3)\n",
    "y1 = np.array([1, 2, 0, 0, 1])\n",
    "S = X1.dot(W1)\n",
    "print \"Training Data:\\n\"\n",
    "print X1\n",
    "print \"\\n Parameters:\"\n",
    "print W1\n",
    "print \"\\nTrue Labels:\"\n",
    "print y1\n",
    "print \"\\nScore (XW):\"\n",
    "print S\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data 0 - Forward Pass\n",
      "\n",
      "[-0.15584511 -1.5508869   2.43319958]\n",
      "-1.55088690054\n",
      "[ 2.39504179  0.          4.98408648]\n",
      "[ 2.39504179  0.          4.98408648]\n",
      "[ 1.  0.  1.]\n",
      "7.37912827505\n",
      "Training data 1 - Forward Pass\n",
      "\n",
      "[-0.60891765  0.82047416 -1.31128664]\n",
      "-1.31128664461\n",
      "[ 1.70236899  3.1317608   0.        ]\n",
      "[ 1.70236899  3.1317608   0.        ]\n",
      "[ 1.  1.  0.]\n",
      "4.83412979912\n",
      "Training data 2 - Forward Pass\n",
      "\n",
      "[ 2.44091895  1.24418953  0.15107114]\n",
      "2.44091894682\n",
      "[ 0.         -0.19672942 -1.2898478 ]\n",
      "[ 0.  0.  0.]\n",
      "[ 0.  0.  0.]\n",
      "0.0\n",
      "Training data 3 - Forward Pass\n",
      "\n",
      "[ 0.47244765  0.30033179 -0.10974783]\n",
      "0.472447649855\n",
      "[ 0.          0.82788414  0.41780452]\n",
      "[ 0.          0.82788414  0.41780452]\n",
      "[ 0.  1.  1.]\n",
      "1.24568865799\n",
      "Training data 4 - Forward Pass\n",
      "\n",
      "[-1.10284687  0.40259238 -1.55471088]\n",
      "0.402592380905\n",
      "[-0.50543925  0.         -0.95730326]\n",
      "[ 0.  0.  0.]\n",
      "[ 0.  0.  0.]\n",
      "0.0\n",
      "\n",
      "These following two numbers should be equal\n",
      "2.69178934643\n",
      "2.69178934643\n",
      "0.344856805919\n",
      "3.03664615235\n",
      "Training data 0 - Backward Pass\n",
      "\n",
      "0.2\n",
      "[ 0.2  0.2  0.2]\n",
      "[ 0.2  0.   0.2]\n",
      "[ 0.2 -0.4  0.2]\n",
      "Training data 1 - Backward Pass\n",
      "\n",
      "0.2\n",
      "[ 0.2  0.2  0.2]\n",
      "[ 0.2  0.2  0. ]\n",
      "[ 0.2  0.2 -0.4]\n",
      "Training data 2 - Backward Pass\n",
      "\n",
      "0.2\n",
      "[ 0.2  0.2  0.2]\n",
      "[ 0.  0.  0.]\n",
      "[-0.  0.  0.]\n",
      "Training data 3 - Backward Pass\n",
      "\n",
      "0.2\n",
      "[ 0.2  0.2  0.2]\n",
      "[ 0.   0.2  0.2]\n",
      "[-0.4  0.2  0.2]\n",
      "Training data 4 - Backward Pass\n",
      "\n",
      "0.2\n",
      "[ 0.2  0.2  0.2]\n",
      "[ 0.  0.  0.]\n",
      "[ 0. -0.  0.]\n",
      "dW (Gradient) Before Regularization:\n",
      "\n",
      "[[ 0.25391582 -0.60414087  0.35022505]\n",
      " [-0.01551153 -0.3358883   0.35139983]\n",
      " [-0.11001949 -0.20957558  0.31959508]\n",
      " [-0.22156894  0.38981305 -0.16824411]]\n",
      "dW (Gradient) after Regularization:\n",
      "\n",
      "[[ 0.22222387 -0.64395181  0.43614482]\n",
      " [ 0.0797974  -0.39951188  0.50834795]\n",
      " [-0.04297649 -0.23001986  0.33949779]\n",
      " [-0.14352709  0.47894588 -0.21173386]]\n",
      "0.05\n",
      "0.05\n",
      "[[-0.03169195 -0.03981094  0.08591976]\n",
      " [ 0.09530892 -0.06362358  0.15694812]\n",
      " [ 0.067043   -0.02044427  0.01990272]\n",
      " [ 0.07804185  0.08913283 -0.04348975]]\n",
      "Outputs of svm_loss_naive:\n",
      "\n",
      "3.03664615235\n",
      "[[ 0.22222387 -0.64395181  0.43614482]\n",
      " [ 0.0797974  -0.39951188  0.50834795]\n",
      " [-0.04297649 -0.23001986  0.33949779]\n",
      " [-0.14352709  0.47894588 -0.21173386]]\n"
     ]
    }
   ],
   "source": [
    "loss, grad = svm_loss_naive(W1, X1, y1, 0.1, True)\n",
    "\n",
    "print \"Outputs of svm_loss_naive:\\n\"\n",
    "print loss\n",
    "print grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run gradient check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Numerical gradient calculation\n",
      "numerical: 0.835477 analytic: 0.835477, relative error: 6.458937e-12\n",
      "numerical: 0.835477 analytic: 0.835477, relative error: 6.458937e-12\n",
      "numerical: 0.835477 analytic: 0.835477, relative error: 6.458937e-12\n"
     ]
    }
   ],
   "source": [
    "from cs231n.gradient_check import grad_check_sparse\n",
    "\n",
    "loss, grad = svm_loss_naive(W1, X1, y1, 0.5, False)\n",
    "\n",
    "print '\\n'+'Numerical gradient calculation' \n",
    "f = lambda w: svm_loss_naive(W1, X1, y1, 0.5, False)[0]\n",
    "grad_numerical = grad_check_sparse(f, W1, grad, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized Implementation\n",
      "Training data %d - Forward Pass\n",
      "\n",
      "Parameters:\n",
      "[[-0.31691947 -0.39810944  0.85919763]\n",
      " [ 0.95308924 -0.63623584  1.56948124]\n",
      " [ 0.67043001 -0.20444275  0.19902715]\n",
      " [ 0.78041851  0.89132826 -0.43489747]]\n",
      "\n",
      "Training Data:\n",
      "[[ 0.57442478  1.0679959  -0.50499719 -0.83689563]\n",
      " [-1.0161851  -0.07748431 -1.38694829  0.09319964]\n",
      " [ 0.25843924  0.39654861  0.8764847   1.99541077]\n",
      " [-0.8556697   0.53403462 -0.67092401  0.18207436]\n",
      " [ 0.02367122 -1.11203876  0.26283394 -0.27124496]]\n",
      "\n",
      "True Labels:\n",
      "[1 2 0 0 1]\n",
      "\n",
      "Score (XW):\n",
      "[[-0.15584511 -1.5508869   2.43319958]\n",
      " [-0.60891765  0.82047416 -1.31128664]\n",
      " [ 2.44091895  1.24418953  0.15107114]\n",
      " [ 0.47244765  0.30033179 -0.10974783]\n",
      " [-1.10284687  0.40259238 -1.55471088]]\n",
      "[-1.5508869  -1.31128664  2.44091895  0.47244765  0.40259238]\n",
      "[ 1.5508869   1.31128664 -2.44091895 -0.47244765 -0.40259238]\n",
      "[[ 2.39504179  0.          4.98408648]\n",
      " [ 1.70236899  3.1317608   0.        ]\n",
      " [ 0.         -0.19672942 -1.2898478 ]\n",
      " [ 0.          0.82788414  0.41780452]\n",
      " [-0.50543925  0.         -0.95730326]]\n",
      "[[ 2.39504179  0.          4.98408648]\n",
      " [ 1.70236899  3.1317608   0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.82788414  0.41780452]\n",
      " [ 0.          0.          0.        ]]\n",
      "[[1 0 1]\n",
      " [1 1 0]\n",
      " [0 0 0]\n",
      " [0 1 1]\n",
      " [0 0 0]]\n",
      "[ 7.37912828  4.8341298   0.          1.24568866  0.        ]\n",
      "Naive Implementation\n",
      "Training data 0 - Forward Pass\n",
      "\n",
      "[-0.15584511 -1.5508869   2.43319958]\n",
      "-1.55088690054\n",
      "[ 2.39504179  0.          4.98408648]\n",
      "[ 2.39504179  0.          4.98408648]\n",
      "[ 1.  0.  1.]\n",
      "7.37912827505\n",
      "Training data 1 - Forward Pass\n",
      "\n",
      "[-0.60891765  0.82047416 -1.31128664]\n",
      "-1.31128664461\n",
      "[ 1.70236899  3.1317608   0.        ]\n",
      "[ 1.70236899  3.1317608   0.        ]\n",
      "[ 1.  1.  0.]\n",
      "4.83412979912\n",
      "Training data 2 - Forward Pass\n",
      "\n",
      "[ 2.44091895  1.24418953  0.15107114]\n",
      "2.44091894682\n",
      "[ 0.         -0.19672942 -1.2898478 ]\n",
      "[ 0.  0.  0.]\n",
      "[ 0.  0.  0.]\n",
      "0.0\n",
      "Training data 3 - Forward Pass\n",
      "\n",
      "[ 0.47244765  0.30033179 -0.10974783]\n",
      "0.472447649855\n",
      "[ 0.          0.82788414  0.41780452]\n",
      "[ 0.          0.82788414  0.41780452]\n",
      "[ 0.  1.  1.]\n",
      "1.24568865799\n",
      "Training data 4 - Forward Pass\n",
      "\n",
      "[-1.10284687  0.40259238 -1.55471088]\n",
      "0.402592380905\n",
      "[-0.50543925  0.         -0.95730326]\n",
      "[ 0.  0.  0.]\n",
      "[ 0.  0.  0.]\n",
      "0.0\n",
      "\n",
      "These following two numbers should be equal\n",
      "2.69178934643\n",
      "2.69178934643\n",
      "0.0\n",
      "2.69178934643\n",
      "Training data 0 - Backward Pass\n",
      "\n",
      "0.2\n",
      "[ 0.2  0.2  0.2]\n",
      "[ 0.2  0.   0.2]\n",
      "[ 0.2 -0.4  0.2]\n",
      "Training data 1 - Backward Pass\n",
      "\n",
      "0.2\n",
      "[ 0.2  0.2  0.2]\n",
      "[ 0.2  0.2  0. ]\n",
      "[ 0.2  0.2 -0.4]\n",
      "Training data 2 - Backward Pass\n",
      "\n",
      "0.2\n",
      "[ 0.2  0.2  0.2]\n",
      "[ 0.  0.  0.]\n",
      "[-0.  0.  0.]\n",
      "Training data 3 - Backward Pass\n",
      "\n",
      "0.2\n",
      "[ 0.2  0.2  0.2]\n",
      "[ 0.   0.2  0.2]\n",
      "[-0.4  0.2  0.2]\n",
      "Training data 4 - Backward Pass\n",
      "\n",
      "0.2\n",
      "[ 0.2  0.2  0.2]\n",
      "[ 0.  0.  0.]\n",
      "[ 0. -0.  0.]\n",
      "dW (Gradient) Before Regularization:\n",
      "\n",
      "[[ 0.25391582 -0.60414087  0.35022505]\n",
      " [-0.01551153 -0.3358883   0.35139983]\n",
      " [-0.11001949 -0.20957558  0.31959508]\n",
      " [-0.22156894  0.38981305 -0.16824411]]\n",
      "dW (Gradient) after Regularization:\n",
      "\n",
      "[[ 0.25391582 -0.60414087  0.35022505]\n",
      " [-0.01551153 -0.3358883   0.35139983]\n",
      " [-0.11001949 -0.20957558  0.31959508]\n",
      " [-0.22156894  0.38981305 -0.16824411]]\n",
      "0.0\n",
      "0.0\n",
      "[[-0. -0.  0.]\n",
      " [ 0. -0.  0.]\n",
      " [ 0. -0.  0.]\n",
      " [ 0.  0. -0.]]\n"
     ]
    }
   ],
   "source": [
    "from cs231n.classifiers.linear_svm_V1 import svm_loss_vectorized\n",
    "\n",
    "print \"Vectorized Implementation\"\n",
    "loss, grad = svm_loss_vectorized(W1, X1, y1, 0.0, True)\n",
    "\n",
    "print \"Naive Implementation\"\n",
    "loss, grad = svm_loss_naive(W1, X1, y1, 0.0, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data 0 - Forward Pass\n",
      "\n",
      "[-0.15584511 -1.5508869   2.43319958]\n",
      "-1.55088690054\n",
      "[ 2.39504179  0.          4.98408648]\n",
      "[ 2.39504179  0.          4.98408648]\n",
      "[ 1.  0.  1.]\n",
      "7.37912827505\n",
      "Training data 1 - Forward Pass\n",
      "\n",
      "[-0.60891765  0.82047416 -1.31128664]\n",
      "-1.31128664461\n",
      "[ 1.70236899  3.1317608   0.        ]\n",
      "[ 1.70236899  3.1317608   0.        ]\n",
      "[ 1.  1.  0.]\n",
      "4.83412979912\n",
      "Training data 2 - Forward Pass\n",
      "\n",
      "[ 2.44091895  1.24418953  0.15107114]\n",
      "2.44091894682\n",
      "[ 0.         -0.19672942 -1.2898478 ]\n",
      "[ 0.  0.  0.]\n",
      "[ 0.  0.  0.]\n",
      "0.0\n",
      "Training data 3 - Forward Pass\n",
      "\n",
      "[ 0.47244765  0.30033179 -0.10974783]\n",
      "0.472447649855\n",
      "[ 0.          0.82788414  0.41780452]\n",
      "[ 0.          0.82788414  0.41780452]\n",
      "[ 0.  1.  1.]\n",
      "1.24568865799\n",
      "Training data 4 - Forward Pass\n",
      "\n",
      "[-1.10284687  0.40259238 -1.55471088]\n",
      "0.402592380905\n",
      "[-0.50543925  0.         -0.95730326]\n",
      "[ 0.  0.  0.]\n",
      "[ 0.  0.  0.]\n",
      "0.0\n",
      "\n",
      "These following two numbers should be equal\n",
      "2.69178934643\n",
      "2.69178934643\n",
      "1.72428402959\n",
      "4.41607337602\n",
      "Training data 0 - Backward Pass\n",
      "\n",
      "0.2\n",
      "[ 0.2  0.2  0.2]\n",
      "[ 0.2  0.   0.2]\n",
      "[ 0.2 -0.4  0.2]\n",
      "Training data 1 - Backward Pass\n",
      "\n",
      "0.2\n",
      "[ 0.2  0.2  0.2]\n",
      "[ 0.2  0.2  0. ]\n",
      "[ 0.2  0.2 -0.4]\n",
      "Training data 2 - Backward Pass\n",
      "\n",
      "0.2\n",
      "[ 0.2  0.2  0.2]\n",
      "[ 0.  0.  0.]\n",
      "[-0.  0.  0.]\n",
      "Training data 3 - Backward Pass\n",
      "\n",
      "0.2\n",
      "[ 0.2  0.2  0.2]\n",
      "[ 0.   0.2  0.2]\n",
      "[-0.4  0.2  0.2]\n",
      "Training data 4 - Backward Pass\n",
      "\n",
      "0.2\n",
      "[ 0.2  0.2  0.2]\n",
      "[ 0.  0.  0.]\n",
      "[ 0. -0.  0.]\n",
      "dW (Gradient) Before Regularization:\n",
      "\n",
      "[[ 0.25391582 -0.60414087  0.35022505]\n",
      " [-0.01551153 -0.3358883   0.35139983]\n",
      " [-0.11001949 -0.20957558  0.31959508]\n",
      " [-0.22156894  0.38981305 -0.16824411]]\n",
      "dW (Gradient) after Regularization:\n",
      "\n",
      "[[ 0.09545608 -0.80319559  0.77982387]\n",
      " [ 0.46103309 -0.65400622  1.13614045]\n",
      " [ 0.22519552 -0.31179696  0.41910865]\n",
      " [ 0.16864031  0.83547718 -0.38569285]]\n",
      "0.25\n",
      "0.25\n",
      "[[-0.15845974 -0.19905472  0.42959881]\n",
      " [ 0.47654462 -0.31811792  0.78474062]\n",
      " [ 0.33521501 -0.10222137  0.09951358]\n",
      " [ 0.39020926  0.44566413 -0.21744873]]\n",
      "4.41607337602\n",
      "Training data %d - Forward Pass\n",
      "\n",
      "Parameters:\n",
      "[[-0.31691947 -0.39810944  0.85919763]\n",
      " [ 0.95308924 -0.63623584  1.56948124]\n",
      " [ 0.67043001 -0.20444275  0.19902715]\n",
      " [ 0.78041851  0.89132826 -0.43489747]]\n",
      "\n",
      "Training Data:\n",
      "[[ 0.57442478  1.0679959  -0.50499719 -0.83689563]\n",
      " [-1.0161851  -0.07748431 -1.38694829  0.09319964]\n",
      " [ 0.25843924  0.39654861  0.8764847   1.99541077]\n",
      " [-0.8556697   0.53403462 -0.67092401  0.18207436]\n",
      " [ 0.02367122 -1.11203876  0.26283394 -0.27124496]]\n",
      "\n",
      "True Labels:\n",
      "[1 2 0 0 1]\n",
      "\n",
      "Score (XW):\n",
      "[[-0.15584511 -1.5508869   2.43319958]\n",
      " [-0.60891765  0.82047416 -1.31128664]\n",
      " [ 2.44091895  1.24418953  0.15107114]\n",
      " [ 0.47244765  0.30033179 -0.10974783]\n",
      " [-1.10284687  0.40259238 -1.55471088]]\n",
      "[-1.5508869  -1.31128664  2.44091895  0.47244765  0.40259238]\n",
      "[ 1.5508869   1.31128664 -2.44091895 -0.47244765 -0.40259238]\n",
      "[[ 2.39504179  0.          4.98408648]\n",
      " [ 1.70236899  3.1317608   0.        ]\n",
      " [ 0.         -0.19672942 -1.2898478 ]\n",
      " [ 0.          0.82788414  0.41780452]\n",
      " [-0.50543925  0.         -0.95730326]]\n",
      "[[ 2.39504179  0.          4.98408648]\n",
      " [ 1.70236899  3.1317608   0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.82788414  0.41780452]\n",
      " [ 0.          0.          0.        ]]\n",
      "[[1 0 1]\n",
      " [1 1 0]\n",
      " [0 0 0]\n",
      " [0 1 1]\n",
      " [0 0 0]]\n",
      "[ 7.37912828  4.8341298   0.          1.24568866  0.        ]\n",
      "2.69178934643\n",
      "1.72428402959\n",
      "Training data - Backward Pass\n",
      "\n",
      "[ 0.2  0.2  0.2  0.2  0.2]\n",
      "[[ 0.2  0.2  0.2]\n",
      " [ 0.2  0.2  0.2]\n",
      " [ 0.2  0.2  0.2]\n",
      " [ 0.2  0.2  0.2]\n",
      " [ 0.2  0.2  0.2]]\n",
      "[[ 0.2  0.   0.2]\n",
      " [ 0.2  0.2  0. ]\n",
      " [ 0.   0.   0. ]\n",
      " [ 0.   0.2  0.2]\n",
      " [ 0.   0.   0. ]]\n",
      "[[ 0.2 -0.4  0.2]\n",
      " [ 0.2  0.2 -0.4]\n",
      " [-0.   0.   0. ]\n",
      " [-0.4  0.2  0.2]\n",
      " [ 0.  -0.   0. ]]\n",
      "(5L, 3L)\n",
      "(5L, 4L)\n",
      "(4L, 3L)\n",
      "dW (Gradient) Before Regularization:\n",
      "\n",
      "[[ 0.25391582 -0.60414087  0.35022505]\n",
      " [-0.01551153 -0.3358883   0.35139983]\n",
      " [-0.11001949 -0.20957558  0.31959508]\n",
      " [-0.22156894  0.38981305 -0.16824411]]\n",
      "dW (Gradient) after Regularization:\n",
      "\n",
      "[[ 0.09545608 -0.80319559  0.77982387]\n",
      " [ 0.46103309 -0.65400622  1.13614045]\n",
      " [ 0.22519552 -0.31179696  0.41910865]\n",
      " [ 0.16864031  0.83547718 -0.38569285]]\n",
      "4.41607337602\n",
      "\n",
      "Numerical gradient calculation\n",
      "numerical: -0.311797 analytic: -0.311797, relative error: 3.760659e-11\n",
      "numerical: -0.311797 analytic: -0.311797, relative error: 3.760659e-11\n",
      "numerical: 1.136140 analytic: 1.136140, relative error: 6.323971e-12\n"
     ]
    }
   ],
   "source": [
    "from cs231n.classifiers.linear_svm_V1 import svm_loss_vectorized\n",
    "\n",
    "loss, grad = svm_loss_naive(W1, X1, y1, 0.5, True)\n",
    "print loss\n",
    "\n",
    "loss, grad = svm_loss_vectorized(W1, X1, y1, 0.5, True)\n",
    "print loss\n",
    "\n",
    "print '\\n'+'Numerical gradient calculation' \n",
    "f = lambda w: svm_loss_vectorized(W1, X1, y1, 0.5, False)[0]\n",
    "grad_numerical = grad_check_sparse(f, W1, grad, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Development code - Softmax\n",
    "\n",
    "We start development by using a smaller and more manageable data set to figure out what a piece of code is actually doing or to test out a new piece of code before applying it to larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.06468274  0.09728655 -0.98904076  1.2305726 ]\n",
      " [ 0.91878574 -1.41662552 -0.17210046  0.21351515]\n",
      " [ 0.84495807  1.19061162 -0.1311505  -0.03793197]\n",
      " [ 1.11992998  0.76793865  0.70910003  2.25192427]\n",
      " [-1.01017972 -1.14507688 -0.81452389 -0.16170988]]\n",
      "float64\n",
      "[[ 0.43184044  0.86656964  0.73077787]\n",
      " [ 0.22107617  1.38770271  0.64829212]\n",
      " [ 0.49143095  1.27202316 -1.04341634]\n",
      " [ 0.20637279 -0.0477959   0.53545559]]\n",
      "float64\n",
      "[1 2 0 0 1]\n",
      "Training data - Forward Pass\n",
      "\n",
      " Scores:\n",
      "[[-0.67035386 -2.104516    0.97592179]\n",
      " [ 0.04307491 -1.39878416  0.04694144]\n",
      " [ 0.55582338  2.2194165   1.50587448]\n",
      " [ 1.46661358  2.83052678  1.78218749]\n",
      " [-1.12304043 -3.49278155 -0.71726223]]\n",
      "\n",
      " Scaling Constant:\n",
      "-0.717262226852\n",
      "\n",
      " Normalized Scores:\n",
      "[[-1.64627565 -3.08043779  0.        ]\n",
      " [-0.00386653 -1.44572561  0.        ]\n",
      " [-1.66359312  0.         -0.71354202]\n",
      " [-1.3639132   0.         -1.04833929]\n",
      " [-0.4057782  -2.77551932  0.        ]]\n",
      "\n",
      " Scores of correct classes:\n",
      "[-3.08043779  0.         -1.66359312 -1.3639132  -2.77551932]\n",
      "\n",
      " Exp(Normalized Scores):\n",
      "[[ 0.1927665   0.04593914  1.        ]\n",
      " [ 0.99614093  0.23557508  1.        ]\n",
      " [ 0.18945701  1.          0.48990586]\n",
      " [ 0.25565837  1.          0.35051938]\n",
      " [ 0.66645797  0.06231711  1.        ]]\n",
      "\n",
      " Sum(Exp(Normalized Scores)):\n",
      "[ 1.23870564  2.23171601  1.67936288  1.60617775  1.72877508]\n",
      "\n",
      " Log(Sum):\n",
      "[ 0.214067    0.8027708   0.51841448  0.47385729  0.54741311]\n",
      "\n",
      " Losses:\n",
      "[ 3.29450479  0.8027708   2.1820076   1.83777049  3.32293243]\n",
      "\n",
      " Data Loss:\n",
      "2.28799722278\n",
      "\n",
      " Reg Loss:\n",
      "0.0\n",
      "\n",
      " Loss:\n",
      "2.28799722278\n",
      "\n",
      " Training data - Backward Pass\n",
      "\n",
      " dloss:\n",
      "[ 0.2  0.2  0.2  0.2  0.2]\n",
      "\n",
      " dcorrect_scores:\n",
      "[-0.2 -0.2 -0.2 -0.2 -0.2]\n",
      "\n",
      " dlog_sums:\n",
      "[ 0.2  0.2  0.2  0.2  0.2]\n",
      "\n",
      " lg_log_sums:\n",
      "[ 0.80729429  0.44808569  0.59546392  0.6225961   0.57844425]\n",
      "\n",
      " sdums:\n",
      "[ 0.16145886  0.08961714  0.11909278  0.12451922  0.11568885]\n",
      "\n",
      " dexp_scores:\n",
      "[[ 0.16145886  0.16145886  0.16145886]\n",
      " [ 0.08961714  0.08961714  0.08961714]\n",
      " [ 0.11909278  0.11909278  0.11909278]\n",
      " [ 0.12451922  0.12451922  0.12451922]\n",
      " [ 0.11568885  0.11568885  0.11568885]]\n",
      "\n",
      " lg_exp_scores:\n",
      "[[ 0.1927665   0.04593914  1.        ]\n",
      " [ 0.99614093  0.23557508  1.        ]\n",
      " [ 0.18945701  1.          0.48990586]\n",
      " [ 0.25565837  1.          0.35051938]\n",
      " [ 0.66645797  0.06231711  1.        ]]\n",
      "\n",
      " dnorm_scores:\n",
      "[[ 0.03112386  0.00741728  0.16145886]\n",
      " [ 0.0892713   0.02111156  0.08961714]\n",
      " [ 0.02256296  0.11909278  0.05834425]\n",
      " [ 0.03183438  0.12451922  0.0436464 ]\n",
      " [ 0.07710176  0.00720939  0.11568885]]\n",
      "\n",
      " dnorm_scores:\n",
      "[[ 0.03112386 -0.19258272  0.16145886]\n",
      " [ 0.0892713   0.02111156 -0.11038286]\n",
      " [-0.17743704  0.11909278  0.05834425]\n",
      " [-0.16816562  0.12451922  0.0436464 ]\n",
      " [ 0.07710176 -0.19279061  0.11568885]]\n",
      "\n",
      " dmax_val:\n",
      "[[ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "\n",
      " dscores:\n",
      "[[ 0.03112386 -0.19258272  0.16145886]\n",
      " [ 0.0892713   0.02111156 -0.11038286]\n",
      " [-0.17743704  0.11909278  0.05834425]\n",
      " [-0.16816562  0.12451922  0.0436464 ]\n",
      " [ 0.07710176 -0.19279061  0.11568885]]\n",
      "dW (Gradient) Before Regularization:\n",
      "\n",
      "[[-0.36726305  0.65927088 -0.29200783]\n",
      " [-0.55212298  0.40953355  0.14258943]\n",
      " [-0.20492291  0.41654891 -0.211626  ]\n",
      " [-0.32707287  0.0745872   0.25248568]]\n",
      "dW (Gradient) after Regularization:\n",
      "\n",
      "[[-0.36726305  0.65927088 -0.29200783]\n",
      " [-0.55212298  0.40953355  0.14258943]\n",
      " [-0.20492291  0.41654891 -0.211626  ]\n",
      " [-0.32707287  0.0745872   0.25248568]]\n",
      "2.28799722278\n",
      "[[-0.36726305  0.65927088 -0.29200783]\n",
      " [-0.55212298  0.40953355  0.14258943]\n",
      " [-0.20492291  0.41654891 -0.211626  ]\n",
      " [-0.32707287  0.0745872   0.25248568]]\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "# Use a smaller dataset to code svm_loss_naive \n",
    "# We will first try a small training dataset with only 5 data points and 3 classes\n",
    "X1 = np.random.randn(5, 4)\n",
    "W1 = np.random.randn(4, 3)\n",
    "\"\"\"\n",
    "X1 = np.array([[1.0,0,2.0,1.0],\n",
    "              [0,1.0,3.0,2.0],\n",
    "              [2.0,3.0,4.0,1.0],\n",
    "              [0,0,2.0,3.0]])\n",
    "W1 = np.array([[2.0,2.0,3.0],\n",
    "              [3.0,3.0,3.0],\n",
    "              [0,0,0],\n",
    "              [1.0,1.0,1.0]])\n",
    "\"\"\"\n",
    "\n",
    "y1 = np.array([1, 2, 0, 0, 1])\n",
    "\n",
    "print X1\n",
    "print X1.dtype\n",
    "print W1\n",
    "print W1.dtype\n",
    "print y1\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "\n",
    "loss, grad = softmax_loss_naive(W1, X1, y1, 0.0, True)\n",
    "print loss\n",
    "print grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 1.103385 analytic: 1.103385, relative error: 1.276301e-11\n",
      "numerical: 0.040793 analytic: 0.040793, relative error: 2.600471e-10\n",
      "numerical: 0.520213 analytic: 0.520213, relative error: 5.028954e-12\n",
      "numerical: 0.520213 analytic: 0.520213, relative error: 5.028954e-12\n",
      "numerical: -0.223886 analytic: -0.223886, relative error: 6.674119e-11\n",
      "numerical: 0.466735 analytic: 0.466735, relative error: 3.222738e-11\n",
      "numerical: -0.223886 analytic: -0.223886, relative error: 6.674119e-11\n",
      "numerical: 1.092556 analytic: 1.092556, relative error: 6.121005e-12\n",
      "numerical: 1.052560 analytic: 1.052560, relative error: 1.495102e-11\n",
      "numerical: 1.052560 analytic: 1.052560, relative error: 1.495102e-11\n"
     ]
    }
   ],
   "source": [
    "from cs231n.gradient_check import grad_check_sparse\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "\n",
    "loss, grad = softmax_loss_naive(W1, X1, y1, 0.5, False)\n",
    "f = lambda w: softmax_loss_naive(w, X1, y1, 0.5, False)[0]\n",
    "grad_numerical = grad_check_sparse(f, W1, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized Implementation\n",
      "2.28799722278\n",
      "[[-0.36726305  0.65927088 -0.29200783]\n",
      " [-0.55212298  0.40953355  0.14258943]\n",
      " [-0.20492291  0.41654891 -0.211626  ]\n",
      " [-0.32707287  0.0745872   0.25248568]]\n",
      "Naive Implementation\n",
      "2.28799722278\n",
      "[[-0.36726305  0.65927088 -0.29200783]\n",
      " [-0.55212298  0.40953355  0.14258943]\n",
      " [-0.20492291  0.41654891 -0.211626  ]\n",
      " [-0.32707287  0.0745872   0.25248568]]\n"
     ]
    }
   ],
   "source": [
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "\n",
    "print \"Vectorized Implementation\"\n",
    "loss, grad = softmax_loss_vectorized(W1, X1, y1, 0.0, False)\n",
    "print loss\n",
    "print grad\n",
    "\n",
    "print \"Naive Implementation\"\n",
    "loss, grad = softmax_loss_naive(W1, X1, y1, 0.0, False)\n",
    "print loss\n",
    "print grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Softmax loss function, naive implementation (with loops)\n",
      "\n",
      "  Inputs have dimension D, there are C classes, and we operate on minibatches\n",
      "  of N examples.\n",
      "\n",
      "  Inputs:\n",
      "  - W: A numpy array of shape (D, C) containing weights.\n",
      "  - X: A numpy array of shape (N, D) containing a minibatch of data.\n",
      "  - y: A numpy array of shape (N,) containing training labels; y[i] = c means\n",
      "    that X[i] has label c, where 0 <= c < C.\n",
      "  - reg: (float) regularization strength\n",
      "\n",
      "  Returns a tuple of:\n",
      "  - loss as single float\n",
      "  - gradient with respect to weights W; an array of same shape as W\n",
      "  \n",
      "\n",
      "  Structured SVM loss function, naive implementation (with loops).\n",
      "\n",
      "  Inputs have dimension D, there are C classes, and we operate on minibatches\n",
      "  of N examples.\n",
      "\n",
      "  Inputs:\n",
      "  - W: A numpy array of shape (D, C) containing weights.\n",
      "  - X: A numpy array of shape (N, D) containing a minibatch of data.\n",
      "  - y: A numpy array of shape (N,) containing training labels; y[i] = c means\n",
      "    that X[i] has label c, where 0 <= c < C.\n",
      "  - reg: (float) regularization strength\n",
      "\n",
      "  Returns a tuple of:\n",
      "  - loss as single float\n",
      "  - gradient with respect to weights W; an array of same shape as W\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "print softmax_loss_naive.__doc__\n",
    "print svm_loss_naive.__doc__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Development code - 2 Layer NN\n",
    "We start development by using a smaller and more manageable data set to figure out what a piece of code is actually doing or to test out a new piece of code before applying it to larger datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
