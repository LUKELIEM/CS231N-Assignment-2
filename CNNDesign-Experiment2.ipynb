{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep CNN Architecture 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# As usual, a bit of setup\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cs231n.classifiers.cnn import *\n",
    "from cs231n.data_utils import get_CIFAR10_data\n",
    "from cs231n.gradient_check import eval_numerical_gradient_array, eval_numerical_gradient\n",
    "from cs231n.layers import *\n",
    "from cs231n.fast_layers import *\n",
    "from cs231n.solver import Solver\n",
    "from cs231n.classifiers.convnet2 import *\n",
    "from cs231n.classifiers.convnet3 import *\n",
    "from time import time\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_val:  (1000L, 3L, 32L, 32L)\n",
      "X_train:  (49000L, 3L, 32L, 32L)\n",
      "X_test:  (1000L, 3L, 32L, 32L)\n",
      "y_val:  (1000L,)\n",
      "y_train:  (49000L,)\n",
      "y_test:  (1000L,)\n"
     ]
    }
   ],
   "source": [
    "# Load the (preprocessed) CIFAR10 data.\n",
    "\n",
    "data = get_CIFAR10_data()\n",
    "for k, v in data.iteritems():\n",
    "  print '%s: ' % k, v.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble of DeepCNN3 Models\n",
    "\n",
    "There is accuracy improvement from ensembling models of DeepCNN3 architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1: \n",
      "Validation set accuracy:  0.811\n",
      "Test set accuracy:  0.803\n",
      "Testing Time: 33.181000s\n",
      "Model 2: \n",
      "Validation set accuracy:  0.869\n",
      "Test set accuracy:  0.86\n",
      "Testing Time: 29.608000s\n",
      "Model 3: \n",
      "Validation set accuracy:  0.849\n",
      "Test set accuracy:  0.85\n",
      "Testing Time: 20.276000s\n",
      "Model 4: \n",
      "Validation set accuracy:  0.874\n",
      "Test set accuracy:  0.864\n",
      "Testing Time: 20.336000s\n",
      "Model 1+2+3+4 Ensemble: \n",
      "Validation set accuracy:  0.889\n",
      "Test set accuracy:  0.883\n",
      "Testing Time: 0.000000s\n"
     ]
    }
   ],
   "source": [
    "outfile = 'bestparams-1.npz'\n",
    "npzfile = np.load(outfile)\n",
    "\n",
    "params = npzfile['params'].item()\n",
    "bn_params = npzfile['bn_params'].item()\n",
    "\n",
    "# This was the model that was trained\n",
    "\n",
    "model_1 = DeepConvNet3(num_filters=[[32,64,64],[128,128,128]], filter_sizes=[[3,3,3],[3,3,3]], weight_scale=1e-2, \n",
    "                    hidden_dim=500, verbose=False, reg = 0, use_batchnorm=True)\n",
    "model_1.params = params  # Transfer parameters from pre-trained model\n",
    "model_1.bn_params = bn_params  # Transfer parameters from pre-trained model\n",
    "\n",
    "t1 = time()\n",
    "score_test_1 = model_1.loss(data['X_test'])\n",
    "y_test_pred = np.argmax(score_test_1, axis=1)\n",
    "t2 = time()\n",
    "score_val_1 = model_1.loss(data['X_val'])\n",
    "y_val_pred = np.argmax(score_val_1, axis=1)\n",
    "\n",
    "print 'Model 1: '\n",
    "print 'Validation set accuracy: ', (y_val_pred == data['y_val']).mean()\n",
    "print 'Test set accuracy: ', (y_test_pred == data['y_test']).mean()\n",
    "print 'Testing Time: %fs' % (t2 - t1)\n",
    "\n",
    "outfile = 'bestparams-2.npz'\n",
    "npzfile = np.load(outfile)\n",
    "\n",
    "params = npzfile['params'].item()\n",
    "bn_params = npzfile['bn_params'].item()\n",
    "\n",
    "# This was the model that was trained\n",
    "\n",
    "model_2 = DeepConvNet3(num_filters=[[32,64,64],[128,128,128]], filter_sizes=[[3,3,3],[3,3,3]], weight_scale=1e-2, \n",
    "                    hidden_dim=500, verbose=False, reg = 0, use_batchnorm=True)\n",
    "model_2.params = params  # Transfer parameters from pre-trained model\n",
    "model_2.bn_params = bn_params  # Transfer parameters from pre-trained model\n",
    "\n",
    "t1 = time()\n",
    "score_test_2 = model_2.loss(data['X_test'])\n",
    "y_test_pred = np.argmax(score_test_2, axis=1)\n",
    "t2 = time()\n",
    "score_val_2 = model_2.loss(data['X_val'])\n",
    "y_val_pred = np.argmax(score_val_2, axis=1)\n",
    "\n",
    "print 'Model 2: '\n",
    "print 'Validation set accuracy: ', (y_val_pred == data['y_val']).mean()\n",
    "print 'Test set accuracy: ', (y_test_pred == data['y_test']).mean()\n",
    "print 'Testing Time: %fs' % (t2 - t1)\n",
    "\n",
    "\n",
    "outfile = 'bestparams-4.npz'\n",
    "npzfile = np.load(outfile)\n",
    "\n",
    "params = npzfile['params'].item()\n",
    "bn_params = npzfile['bn_params'].item()\n",
    "\n",
    "# This was the model that was trained\n",
    "\n",
    "model_3 = DeepConvNet3(num_filters=[[64,64],[128,128,128]], filter_sizes=[[3,3],[3,3,3]], weight_scale=1e-2, \n",
    "                    hidden_dim=500, verbose=False, reg = 0, use_batchnorm=True)\n",
    "model_3.params = params  # Transfer parameters from pre-trained model\n",
    "model_3.bn_params = bn_params  # Transfer parameters from pre-trained model\n",
    "\n",
    "t1 = time()\n",
    "score_test_3 = model_3.loss(data['X_test'])\n",
    "y_test_pred = np.argmax(score_test_3, axis=1)\n",
    "t2 = time()\n",
    "score_val_3 = model_3.loss(data['X_val'])\n",
    "y_val_pred = np.argmax(score_val_3, axis=1)\n",
    "\n",
    "print 'Model 3: '\n",
    "print 'Validation set accuracy: ', (y_val_pred == data['y_val']).mean()\n",
    "print 'Test set accuracy: ', (y_test_pred == data['y_test']).mean()\n",
    "print 'Testing Time: %fs' % (t2 - t1)\n",
    "\n",
    "\n",
    "outfile = 'bestparams-5.npz'\n",
    "npzfile = np.load(outfile)\n",
    "\n",
    "params = npzfile['params'].item()\n",
    "bn_params = npzfile['bn_params'].item()\n",
    "\n",
    "# This was the model that was trained\n",
    "\n",
    "model_4 = DeepConvNet3(num_filters=[[64,64],[128,128,128]], filter_sizes=[[3,3],[3,3,3]], weight_scale=1e-2, \n",
    "                    hidden_dim=500, verbose=False, reg = 0, use_batchnorm=True)\n",
    "model_4.params = params  # Transfer parameters from pre-trained model\n",
    "model_4.bn_params = bn_params  # Transfer parameters from pre-trained model\n",
    "\n",
    "t1 = time()\n",
    "score_test_4 = model_4.loss(data['X_test'])\n",
    "y_test_pred = np.argmax(score_test_4, axis=1)\n",
    "t2 = time()\n",
    "score_val_4 = model_4.loss(data['X_val'])\n",
    "y_val_pred = np.argmax(score_val_4, axis=1)\n",
    "\n",
    "print 'Model 4: '\n",
    "print 'Validation set accuracy: ', (y_val_pred == data['y_val']).mean()\n",
    "print 'Test set accuracy: ', (y_test_pred == data['y_test']).mean()\n",
    "print 'Testing Time: %fs' % (t2 - t1)\n",
    "\n",
    "t1 = time()\n",
    "y_test_pred = np.argmax((score_test_1+score_test_2+score_test_3+score_test_4)/4.0, axis=1)\n",
    "t2 = time()\n",
    "y_val_pred = np.argmax((score_val_1+score_val_2+score_val_3+score_val_4)/4.0, axis=1)\n",
    "\n",
    "print 'Model 1+2+3+4 Ensemble: '\n",
    "print 'Validation set accuracy: ', (y_val_pred == data['y_val']).mean()\n",
    "print 'Test set accuracy: ', (y_test_pred == data['y_test']).mean()\n",
    "print 'Testing Time: %fs' % (t2 - t1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1+2+3+4 Ensemble: \n",
      "Validation set accuracy:  0.89\n",
      "Test set accuracy:  0.886\n",
      "Testing Time: 0.000000s\n"
     ]
    }
   ],
   "source": [
    "t1 = time()\n",
    "y_test_pred = np.argmax((score_test_2+score_test_4)/2.0, axis=1)\n",
    "t2 = time()\n",
    "y_val_pred = np.argmax((score_val_2+score_val_4)/2.0, axis=1)\n",
    "\n",
    "print 'Model 1+2+3+4 Ensemble: '\n",
    "print 'Validation set accuracy: ', (y_val_pred == data['y_val']).mean()\n",
    "print 'Test set accuracy: ', (y_test_pred == data['y_test']).mean()\n",
    "print 'Testing Time: %fs' % (t2 - t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
